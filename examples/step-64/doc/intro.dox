<br>

<i>
This program was contributed by Bruno Turcksin, Daniel Arndt, Oak Ridge National Laboratory.
</i>


<h1>Introduction</h1>

This example shows how to implement a matrix-free method on the GPU using CUDA
for the Helmhotz equation with variable coefficients on a hypercube. The linear
system will be solved using the conjugate gradient method and is parallelized
 with MPI.

In the last few years, heterogeneous computing in general and GPUs in particular
have gained a lot of popularity. This is because GPUs offer better computing
capabilities and memory bandwidth than CPUs for a given power budget.
Among the architectures available in early 2019, GPUs are about 2x-3x as power
efficient than server CPUs with wide SIMD for PDE-related tasks. GPUs are also
the most popular architecture for machine learning. Therefore, it might be
interesting to be able to efficiently run a simulation alongside a machine
learning code.

While we have tried for the interface of the matrix-free classes for the CPU and
the GPU to be as close as possible, there are a few differences. When using
the matrix-free framework on a GPU, one must write some CUDA code. However, the
amount is fairly small and the use of CUDA is limited to a few keywords.

<h3>The test case</h3>

In this example, we consider the Helmholtz problem @f{eqnarray*} - \nabla \cdot
\nabla u + a(\mathbf r) u &=&1,\\ u &=& 0 \quad \text{on} \partial \Omega @f}
where $a(\mathbf x)$ is a variable coefficient.

We choose as domain $\Omega=[0,1]^3$ and $a(\mathbf x)=\frac{10}{0.05 +
2\|mathbf r\|^2}$, Since the coefficient is symmetric around the origin but
the domain is not, we will end up with a non-symmetric solution.

<h3>Moving data to and from the device</h3>

GPUs (we will use device from now on to refer to the GPU) have their own memory
that is separate from the memory accessible to the CPU (we will use the term
"host" from now on). A normal calculation on the device can be divided in three
separate steps:
 1) the data is moved from the host to the device
 2) the computation is done on the device
 3) the result is moved back from the device to the host
The data movements can either be done explicitly by the user code or done
automatically using UVM (Unified Virtual Memory). In deal.II, only the first
method is supported. While it means an extra burden for the user, it allows a
better control of data movement and more importantly it avoids to mistakenly run
important kernels on the host instead of the device.

The data movement in deal.II is done using
LinearAlgebra::ReadWriteVector<Number>. These vectors can be seen as buffers on
the host that are used to either store data from the device or to send data to
the device. There are two types of vectors that can be used on the device:
LinearAlgebra::CUDAWrappers::Vector<Number>, which is similar to the more common
Vector<Number>, and LinearAlgebra::distributed::Vector<Number,
MemorySpace::CUDA>, which is a regular
LinearAlgebra::distributed::Vector<Number> where we have specified which memory
space to use. If no memory space is specified, the default is MemorySpace::Host.

Next, we show how to move data to/from the device using
LinearAlgebra::CUDAWrappers::Vector<Number>:
<code>
unsigned int size = 10;
LinearAlgebra::CUDAWrappers::Vector<double> vector_dev(10);
LinearAlgebra::ReadWriteVector<double> rw_vector(10);
// Fill rw_vector...
// Move the data to the device.
vector_dev.import(rw_vector, VectorOperations::insert);
// Do some computations on the device
// Move the data to the host
rw_vector.import(vector_dev, VectorOperations::insert);
</code>
Using LinearAlgebra::distributed::Vector<Number, MemorySpace::CUDA> is similar
but the import() stage may involve MPI communication:
<code>
IndexSet locally_owned_dofs, locally_relevant_dofs;
// Fill the two IndexSet...
LinearAlgebra::distributed::Vector<double, MemorySpace::CUDA>
distributed_vector_dev(locally_owned_dofs, MPI_COMM_WORLD);
// Create the ReadWriteVector using an IndexSet instead of the size
LinearAlgebra::ReadWriteVector<double> owned_rw_vector(locally_owned_dofs);
// Fill owned_rw_vector
// Move the data to the device
distributed_vector_dev.import(owned_rw_vector, VectorOperations::insert);
// Do some computations on the device
// Create a ReadWriteVector with a different IndexSet
LinearAlgebra::ReadWriteVector<double> relevant_rw_vector(locally_relevant_dofs);
// Move the data to the host and do an MPI communication
relevnt_rw_vector(distributed_vector_dev, VectorOperations::insert);
</code>

import() supports two kinds of strategies: VectorOperations::insert and
VectorOperations::add.


<h3>Matrix-vector product implementation</h3>

The code necessary to evaluate the matrix-free operator on the device is very
similar to the one on the host. However, there are a few differences, the main
ones being that the local_apply() function in Step-37 and the loop over
quadrature points both need to be encapsulated in their own functors.
